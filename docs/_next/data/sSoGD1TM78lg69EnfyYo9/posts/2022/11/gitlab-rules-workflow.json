{"pageProps":{"posts":[{"title":"プロキシ環境でKubernetes構築（Containerd+Calico）","date":"2022-12-27T00:00:00.000Z","ref":"/posts/2022/12/kubernetes-behind-proxy","desc":"\n同期と一緒にトラシュしたので、プロキシ環境下で kubeadm + Containerd + Calico の Kubernetes クラスターを構築する方法について記録を残します。\n\n## 環境\n\n* Ubuntu 22.04\n  * サーバーはニフクラを利用（e-medium4 2vCPU/4GB）\n* Kubernetes v1.26.0\n* kubeadm v1.26.0\n* Containerd v1.6.14\n* Calico v3.24.5\n\nコントロールプレーン、ノード1台ずつの構成とします。プロキシを経由しなければインターネットに出られないようになっています。\n\n![net","draft":false,"content":"\n同期と一緒にトラシュしたので、プロキシ環境下で kubeadm + Containerd + Calico の Kubernetes クラスターを構築する方法について記録を残します。\n\n## 環境\n\n* Ubuntu 22.04\n  * サーバーはニフクラを利用（e-medium4 2vCPU/4GB）\n* Kubernetes v1.26.0\n* kubeadm v1.26.0\n* Containerd v1.6.14\n* Calico v3.24.5\n\nコントロールプレーン、ノード1台ずつの構成とします。プロキシを経由しなければインターネットに出られないようになっています。\n\n![network](//www.plantuml.com/plantuml/png/SoWkIImgAStDuSehJybCJ5Uevb9Go4ijASylobP8pybFIim12O51KNvfIMgHDP1NYwIee2YpBB4a5QugCIMbABMuMC5MGSdGt4ZFs53FGCz0tz1CoPeBnHo5Q6mg3PLYhQ7AalFpIehoSmfo4lDIOM9v-Icf40VKSZcavgK07Gu0)\n\nプライベートネットワークの CIDR は `172.31.0.0/16`、プロキシは `http://172.31.0.1:3128` として進めます。\n\nService CIDR はデフォルトの `10.96.0.0/12` を、Pod Network CIDR は Calico のデフォルトである `192.168.0.0/16` を使います。\n\n💡 筆者の環境ではこの記事で紹介する内容で構築できましたが、環境によって状況が異なる可能性があります。プロキシ配下での構築に挑戦する前に、**まずは似た環境のインターネット接続があるサーバーで試すことをおすすめします**。\n\n## プロキシの設定が必要な箇所\n\n### 環境変数（`~/.bashrc`）\n\n#### コントロールプレーン\n\n```sh\nexport HTTP_PROXY=http://172.31.0.1:3128\nexport HTTPS_PROXY=http://172.31.0.1:3128\nexport NO_PROXY=localhost,127.0.0.1,172.31.0.0/16,10.96.0.0/12,192.168.0.0/16\n```\n\nコントロールプレーンでは `NO_PROXY` に Sercice CIDR の IP レンジ（デフォルトは `10.96.0.0/12`）と Pod Network CIDR の IP レンジを設定しておくことで `kubeadm init` 時の preflight check の WARNING を抑えることができます[^1]。\n\n反映するには `source ~/.bashrc` します。\n\n[^1]: 以前は IP レンジを指定できなかったようですが、今は問題なく使えます。https://github.com/kubernetes/kubeadm/issues/324#issuecomment-331483277\n\n#### ノード\n\n```sh\nexport HTTP_PROXY=http://172.31.0.1:3128\nexport HTTPS_PROXY=http://172.31.0.1:3128\nexport NO_PROXY=localhost,127.0.0.1,172.31.0.0/16\n```\n\nノードでは `kubeadm init` を実行しないのでこれだけで OK。\n\n### `/etc/apt/apt.conf`\n\n各種ライブラリのインストールのために必要です。\n\n```\nAcquire::http::proxy \"http://172.31.0.1:3128\";\nAcquire::https::proxy \"http://172.31.0.1:3128\";\n```\n\n### `/etc/systemd/system/containerd.service.d/http-proxy.conf`\n\n```\n[Service]\nEnvironment=\"HTTP_PROXY=http://172.31.0.1:3128\"\nEnvironment=\"HTTPS_PROXY=http://172.31.0.1:3128\"\nEnvironment=\"NO_PROXY=localhost,127.0.0.1,172.31.0.0/16,10.96.0.0/12\"\n```\n\n`NO_PROXY` に Pod Network CIDR の IP レンジも追加しようかと思ったのですが、ノードを跨いだ Pod 間の通信など試した範囲では追加しなくても異常がなかったので追加していません。  \n筆者の Kubernetes の知識が浅いだけかもしれないので、検証不足でしたら教えていただけるとありがたいです。🙇\n\nこのファイルを変更したら Containerd の再起動が必要です。\n\n```sh\nsystemctl daemon-reload\nsystemctl restart containerd\n```\n\n## 構築手順\n\n上記のプロキシ設定以外は通常の手順と同じです。\n\n* [kubeadmを使用したクラスターの作成](https://kubernetes.io/ja/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/)\n* [kubeadmのインストール](https://kubernetes.io/ja/docs/setup/production-environment/tools/kubeadm/install-kubeadm/)\n* [CRIのインストール](https://kubernetes.io/ja/docs/setup/production-environment/container-runtimes/)\n\nUbuntu 22.04 で構築する際の注意点については [Ubuntu 22.04でのKubernetesクラスター構築（ContainerdとSystemdCgroup）](/posts/2022/12/kubernetes-ubuntu22.04-cgroup-systemd) をご参照ください。\n\n```sh\nkubeadm init --pod-network-cidr 192.168.0.0/16\n```\n\n### ネットワークプラグインの適用\n\n構築が完了したらネットワークプラグインを適用します。以下は Calico を使う場合のコマンドです。\n\n```sh\ncurl https://raw.githubusercontent.com/projectcalico/calico/v3.24.5/manifests/calico.yaml -O\nkubectl apply -f calico.yaml\n```\n\n`kubectl -n kube-system get po -w` して、1つの `calico-kube-controllers` とノード数分の `calico-node` Pod が Running なら問題ありません。CoreDNS がクラッシュするバグを踏んだら [KubernetesでCoreDNSがループしてしまう問題への対処](/posts/2022/12/kubernetes-coredns-loop) の記事を参考にしてみてください。\n\n## 動作確認\n\n簡単な動作確認をしてみます。\n\n```sh\nkubectl create deployment nginx --image=nginx\n```\n\n```sh\nPOD_NAME=$(kubectl get pods -l app=nginx -o jsonpath=\"{.items[0].metadata.name}\")\n```\n\n別のシェルを開き、`curl localhost:8080` を試してみます。\n\n```sh\nkubectl port-forward $POD_NAME 8080:80\n```\n\nログを見てみます。\n\n```sh\nkubectl logs $POD_NAME\n```\n\nexec を試します。\n\n```sh\nkubectl exec -ti $POD_NAME -- nginx -v\n```\n\nNodePort Service を試します。\n\n```sh\nkubectl expose deployment nginx --port 80 --type NodePort\n```\n\n```sh\nNODE_PORT=$(kubectl get svc nginx \\\n  --output=jsonpath='{range .spec.ports[0]}{.nodePort}')\n```\n\n```sh\ncurl 127.0.0.1:$NODE_PORT\n```\n\n```sh\nkubectl delete svc nginx\n```\n\n次に Cluster IP Service を試します。\n\n```sh\nkubectl expose deployment nginx --port 80 --type ClusterIP\n```\n\nクライアントを想定した Pod を建てます（nginx ですが）。\n\n```sh\nkubectl create deployment client --image=nginx\n```\n\n```sh\nPOD_NAME_CLIENT=$(kubectl get pods -l app=client -o jsonpath=\"{.items[0].metadata.name}\")\n```\n\nPod 内で `curl nginx` を実行します。\n\n```sh\nkubectl exec -ti $POD_NAME_CLIENT -- curl nginx\n```\n\n`>Welcome to nginx!` の HTML が返ってこれば OK！\n\n## トラブルシューティング集\n\n### `FailedCreatePodSandBox` - ホストのプライベート IP にアクセス不可\n\n`calico-kube-controllers` が ContainerCreating で止まってしまう問題が発生しました。\n\n```\nEvents:\n  Type     Reason                  Age                 From               Message\n  ----     ------                  ----                ----               -------\n  Warning  FailedScheduling        115s                default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling..\n  Normal   Scheduled               104s                default-scheduler  Successfully assigned kube-system/calico-kube-controllers-7bdbfc669-97wdp to controlplane\n  Warning  FailedCreatePodSandBox  103s                kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"6a029253befac6840d358f8f78b865510bb3874b971fc7241d4ded6b1e92ce2d\": plugin type=\"calico\" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/\n  Normal   SandboxChanged          16s (x3 over 103s)  kubelet            Pod sandbox changed, it will be killed and re-created.\n```\n\n`stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/` のエラーメッセージが出ていますが、実際にはマウントはできていました。\n\n「マウントはできて nodename（ホスト名）は取得できているが、プロキシに阻まれて通信できていないのでは？」と考え、Containerd の NO_PROXY の設定にホストのプライベートネットワークの CIDR を追記したら `172.31.0.0/16` たらこの問題は解決しました。\n\n```\n# vim /etc/systemd/system/containerd.service.d/http-proxy.conf\n```\n\n`172.31.0.0/16` を追記。\n\n```\n[Service]\nEnvironment=\"HTTP_PROXY=http://172.31.0.1:3128\"\nEnvironment=\"HTTPS_PROXY=http://172.31.0.1:3128\"\nEnvironment=\"NO_PROXY=localhost,127.0.0.1,172.31.0.0/16\"\n```\n\n```\n# systemctl daemon-reload\n# systemctl restart containerd\n# kubectl -n kube-system delete po calico-kube-controllers-7bdbfc669-97wdp --force\nWarning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\npod \"calico-kube-controllers-7bdbfc669-97wdp\" force deleted\n```\n\n### `FailedCreatePodSandBox` - Service の IP にアクセス不可\n\n状況は変わったものの、こちらも `calico-kube-controllers` が ContainerCreating で止まってしまう問題です。\n\n```\nEvents:\n  Type     Reason                  Age                From               Message\n  ----     ------                  ----               ----               -------\n  Normal   Scheduled               3m12s              default-scheduler  Successfully assigned kube-system/calico-kube-controllers-7bdbfc669-9gltp to controlplane\n  Warning  FailedCreatePodSandBox  72s                kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"68070146a6bd3c3a3044cbe84495c39ef3abafd069f171db2a185c8925aee2d1\": plugin type=\"calico\" failed (add): error getting ClusterInformation: Get \"https://10.96.0.1:443/apis/crd.projectcalico.org/v1/clusterinformations/default\": Service Unavailable\n  Normal   SandboxChanged          12s (x2 over 72s)  kubelet            Pod sandbox changed, it will be killed and re-created.\n```\n\nこちらはエラーメッセージの通りなので、Service CIDR を Containerd の NO_PROXY に追加します。\n\n```\n# vim /etc/systemd/system/containerd.service.d/http-proxy.conf\n```\n\n`10.96.0.0/12` を追記。\n\n```\n[Service]\nEnvironment=\"HTTP_PROXY=http://172.31.0.1:3128\"\nEnvironment=\"HTTPS_PROXY=http://172.31.0.1:3128\"\nEnvironment=\"NO_PROXY=localhost,127.0.0.1,172.31.0.0/16,10.96.0.0/12\"\n```\n\n先ほどと同じように daemon-reload, restart containerd, Pod の強制削除をします。\n\n## まとめ\n\n最後までお読みいただきありがとうございます。\n\nプロキシ環境での Kubernetes クラスター構築は、通常のクラスター構築よりも Kubernetes のネットワーク周りの知識が要求されるので少しハードルが上がります。\n\n冒頭にも書きましたが、まずはインターネットに直接繋がる環境で構築を試してみて、その後にプロキシ環境での構築を実施すると原因の切り分けがスムーズになると思います。\n\n## 参考文献\n\n* [Installing kubernetes behind a corporate proxy](https://medium.com/@vivekanand.poojari/installing-kubernetes-behind-a-corporate-proxy-bc5582e43fb8)\n* [kubeadmを使用したクラスターの作成](https://kubernetes.io/ja/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/)\n* [kubeadmのインストール](https://kubernetes.io/ja/docs/setup/production-environment/tools/kubeadm/install-kubeadm/)\n* [CRIのインストール](https://kubernetes.io/ja/docs/setup/production-environment/container-runtimes/)\n* [Install Calico networking and network policy for on-premises deployments](https://projectcalico.docs.tigera.io/getting-started/kubernetes/self-managed-onprem/onpremises)\n","tags":[{"name":"Kubernetes","ref":"/tags/kubernetes"}]},{"title":"Ubuntu 22.04でのKubernetesクラスター構築（ContainerdとSystemdCgroup）","date":"2022-12-26T00:00:00.000Z","ref":"/posts/2022/12/kubernetes-ubuntu22.04-cgroup-systemd","desc":"\n公式ドキュメントのコマンドを手順通り流し込めば割と簡単に構築できる Kubernetes クラスターですが、Ubuntu 22.04 になってから少し手を入れる必要が出てきたので差分を紹介しておきます。\n\n## 環境\n\n* Ubuntu 22.04\n* Kubernetes v1.26.0\n* kubeadm v1.26.0\n* Containerd v1.6.14\n\n## 何が変わった？\n\nUbuntu 21.10 以降、Cgroup v2 がデフォルトになりました[^1]。  \nCgroup について詳しく知りたい方は [第37回 Linuxカーネルのコンテナ機能 ― cgroupの改良","draft":false,"content":"\n公式ドキュメントのコマンドを手順通り流し込めば割と簡単に構築できる Kubernetes クラスターですが、Ubuntu 22.04 になってから少し手を入れる必要が出てきたので差分を紹介しておきます。\n\n## 環境\n\n* Ubuntu 22.04\n* Kubernetes v1.26.0\n* kubeadm v1.26.0\n* Containerd v1.6.14\n\n## 何が変わった？\n\nUbuntu 21.10 以降、Cgroup v2 がデフォルトになりました[^1]。  \nCgroup について詳しく知りたい方は [第37回 Linuxカーネルのコンテナ機能 ― cgroupの改良版cgroup v2［1］](https://gihyo.jp/admin/serial/01/linux_containers/0037)の記事がわかりやすいので読んでみてください。\n\nKubernetes においては「cgroupドライバー」を kubelet の設定で選択します。`cgroupfs` ドライバーが v1 に、`systemd` ドライバーが v2 に対応していると考えれば問題ないと思います。\n\n[^1]: https://wiki.ubuntu.com/UbuntuWeeklyNewsletter/Issue697\n\n## 何もしないとどうなる？\n\nコンテナは起動しますが、使い物にならないくらい不安定になり再作成を繰り返します。システムコンポーネントのコンテナ（kube-apiserver）も例外でないので、kubectl を叩いてもレスポンスが返ってこなかったり。。\n\n## 解決方法\n\nCgroup v1 に戻す、というやり方もあるとは思うのですが、systemd を Cgroup ドライバーとして設定すれば Cgroup v2 のままで使えるようになるので、新しくクラスターを構築するのであればそうするのがおすすめです。\n\nkubelet に関しては kubeadm v1.22 以降デフォルトで `systemd` を選択するようになったので[^2]、特に気にする必要はないです。\n\n[^2]: 備考: v1.22では、ユーザーがKubeletConfigurationのcgroupDriverフィールドを設定していない場合、kubeadmはデフォルトでsystemdを設定するようになりました。  \nhttps://kubernetes.io/ja/docs/tasks/administer-cluster/kubeadm/configure-cgroup-driver/#kubelet-cgroup%E3%83%89%E3%83%A9%E3%82%A4%E3%83%90%E3%83%BC%E3%81%AE%E8%A8%AD%E5%AE%9A\n\n**Containerd を CRI として使用する場合、Containerd 側でも Cgroup ドライバーを選択する必要があります（今回の記事のミソ）。**\n\n[Containerd のインストール手順](https://kubernetes.io/ja/docs/setup/production-environment/container-runtimes/#containerd%E3%81%AE%E3%82%A4%E3%83%B3%E3%82%B9%E3%83%88%E3%83%BC%E3%83%AB)に下記のコマンドがありますが、その下にさらに大事なことが書かれています[^3]。\n\n```sh\n# containerdの設定\nmkdir -p /etc/containerd\ncontainerd config default | sudo tee /etc/containerd/config.toml\n# containerdの再起動\nsystemctl restart containerd\n```\n\n> `systemd`のcgroupドライバーを使うには、`/etc/containerd/config.toml`内で`plugins.cri.systemd_cgroup = true`を設定してください。\n\n[^3]: 引用で省略した _kubeadmを使う場合は[kubeletのためのcgroupドライバー](https://kubernetes.io/ja/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#%E3%82%B3%E3%83%B3%E3%83%88%E3%83%AD%E3%83%BC%E3%83%AB%E3%83%97%E3%83%AC%E3%83%BC%E3%83%B3%E3%83%8E%E3%83%BC%E3%83%89%E3%81%AEkubelet%E3%81%AB%E3%82%88%E3%81%A3%E3%81%A6%E4%BD%BF%E7%94%A8%E3%81%95%E3%82%8C%E3%82%8Bcgroup%E3%83%89%E3%83%A9%E3%82%A4%E3%83%90%E3%83%BC%E3%81%AE%E8%A8%AD%E5%AE%9A)を手動で設定してください。_ の部分に関してはリンク先で Docker を CRI に使う場合のことにしか実質触れていないので気にしなくていいです。\n\nなので、言われた通り `plugins.cri.systemd_cgroup = true` を設定してから restart をかけるようにしましょう。\n\n```sh\nsed -i 's/SystemdCgroup \\= false/SystemdCgroup \\= true/g' /etc/containerd/config.toml\n```\n\n他の手順は参考文献の上3つのリンク先に記載の手順を実施すれば OK です。\n\nネットワークプラグイン設定後、CoreDNS がクラッシュするバグを踏んだら [KubernetesでCoreDNSがループしてしまう問題への対処](/posts/2022/12/kubernetes-coredns-loop) の記事を参考にしてみてください。\n\n## 参考文献\n\n* [kubeadmを使用したクラスターの作成](https://kubernetes.io/ja/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/)\n* [kubeadmのインストール](https://kubernetes.io/ja/docs/setup/production-environment/tools/kubeadm/install-kubeadm/)\n* [CRIのインストール](https://kubernetes.io/ja/docs/setup/production-environment/container-runtimes/)\n* [cgroupドライバーの設定](https://kubernetes.io/ja/docs/tasks/administer-cluster/kubeadm/configure-cgroup-driver/)\n* [Ubuntu 22.04でkubeadmでKubernetesクラスターが動かない？](https://tech.virtualtech.jp/entry/2022/06/08/115030)\n","tags":[{"name":"Kubernetes","ref":"/tags/kubernetes"}]},{"title":"KubernetesでCoreDNSがループしてしまう問題への対処","date":"2022-12-24T00:00:00.000Z","ref":"/posts/2022/12/kubernetes-coredns-loop","desc":"\n1年前にも Kubernetes クラスターを自力で組んでトラブルシューティングしてみる【The Hard Way】の記事の中で軽く解説したネタです。\n\n## 環境\n\n* Ubuntu 22.04\n* Kubernetes v1.26.0\n* kubeadm v1.26.0\n\n## 問題\n\nKubernetes クラスター内で名前解決に使われる CoreDNS の Pod が `CrashLoopBackOff` になってしまい再起動を繰り返す問題が発生することがあります。\n\n```\n","draft":false,"content":"\n1年前にも [Kubernetes クラスターを自力で組んでトラブルシューティングしてみる【The Hard Way】](https://qiita.com/SogoK/items/192d475c20e07dd38984)の記事の中で軽く解説したネタです。\n\n## 環境\n\n* Ubuntu 22.04\n* Kubernetes v1.26.0\n* kubeadm v1.26.0\n\n## 問題\n\nKubernetes クラスター内で名前解決に使われる CoreDNS の Pod が `CrashLoopBackOff` になってしまい再起動を繰り返す問題が発生することがあります。\n\n```\n# kubectl -n kube-system logs coredns-787d4945fb-6kb5t\n.:53\n[INFO] plugin/reload: Running configuration SHA512 = 591cf328cccc12bc490481273e738df59329c62c0b729d94e8b61db9961c2fa5f046dd37f1cf888b953814040d180f52594972691cd6ff41be96639138a43908\nCoreDNS-1.9.3\nlinux/amd64, go1.18.2, 45b0a11\n[FATAL] plugin/loop: Loop (127.0.0.1:34129 -> :53) detected for zone \".\", see https://coredns.io/plugins/loop#troubleshooting. Query: \"HINFO 1033981844954998931.6641498229839068582.\"\n```\n\n## 原因\n\nhttps://coredns.io/plugins/loop/#troubleshooting\n\nUbuntu 18.04 以降などの最近のディストリビューションで、ローカルの DNS スタブリゾルバ (systemd-resolved) が使われるようになったことが原因です。Kubelet の設定ファイル `/var/lib/kubelet/config.yaml` を見るとデフォルトでは下記のように設定されています。\n\n```conf\nresolvConf: /run/systemd/resolve/resolv.conf\n```\n\nここで指定されているファイル `/run/systemd/resolve/resolv.conf` が kubelet によって、CoreDNS コンテナに渡されます。デフォルトでは中身は以下のようになっているはずです。\n\n```conf\n# This is /run/systemd/resolve/resolv.conf managed by man:systemd-resolved(8).\n# Do not edit.\n#\n# This file might be symlinked as /etc/resolv.conf. If you're looking at\n# /etc/resolv.conf and seeing this text, you have followed the symlink.\n#\n# This is a dynamic resolv.conf file for connecting local clients directly to\n# all known uplink DNS servers. This file lists all configured search domains.\n#\n# Third party programs should typically not access this file directly, but only\n# through the symlink at /etc/resolv.conf. To manage man:resolv.conf(5) in a\n# different way, replace this symlink by a static file or a different symlink.\n#\n# See man:systemd-resolved.service(8) for details about the supported modes of\n# operation for /etc/resolv.conf.\n\nnameserver 127.0.0.1\nsearch .\n```\n\nOS 上ではスタブリゾルバがいるので `127.0.0.1` を DNS とするように設定されていますが、CoreDNS のコンテナ内ではそのコンテナ自身を指すことになります。結果的にループしてしまい、クラッシュします。\n\n## 解決方法\n\n原因は上記の通りなので、`127.0.0.1` ではない外部の DNS サーバーを設定すれば解決できます。\n\n`/run/systemd/resolve/resolv.conf` は systemd-resolved の管理下にある自動生成ファイルですので、大元の設定ファイル `/etc/systemd/resolved.conf` を修正し、systemd-resolved を再起動することで `/run/systemd/resolve/resolv.conf` を再生成させます。\n\nその後新しい `resolv.conf` の内容を反映させるために kubelet と Container runtime (containerd) を再起動します。\n\n```\nNAMESERVER_ADDRESSES=8.8.8.8\nsed -i \"s/^DNS=127.0.0.1$/DNS=${NAMESERVER_ADDRESSES}/\" /etc/systemd/resolved.conf\nsystemctl restart systemd-resolved\nsystemctl restart kubelet\nsystemctl restart containerd\n```\n\nCoreDNS が無事 Running になれば OK です。\n\n```\nroot@controlplane:~# kubectl -n kube-system get po -w\nNAME                                      READY   STATUS             RESTARTS     AGE\ncalico-kube-controllers-7bdbfc669-w4xxt   1/1     Running            0            2m57s\ncalico-node-2f2qc                         1/1     Running            0            15m\ncoredns-787d4945fb-6kb5t                  0/1     CrashLoopBackOff   5 (7s ago)   16m\ncoredns-787d4945fb-q5bhz                  0/1     CrashLoopBackOff   5 (7s ago)   16m\netcd-controlplane                         1/1     Running            0            16m\nkube-apiserver-controlplane               1/1     Running            0            16m\nkube-controller-manager-controlplane      1/1     Running            0            16m\nkube-proxy-692c8                          1/1     Running            0            16m\nkube-scheduler-controlplane               1/1     Running            0            16m\ncoredns-787d4945fb-6kb5t                  0/1     Running            6 (19s ago)   16m\ncoredns-787d4945fb-6kb5t                  1/1     Running            6 (19s ago)   16m\ncoredns-787d4945fb-q5bhz                  0/1     Running            6 (25s ago)   16m\ncoredns-787d4945fb-q5bhz                  1/1     Running            6 (25s ago)   16m\n```\n","tags":[{"name":"Kubernetes","ref":"/tags/kubernetes"}]},{"title":"VS Code Serverでリモートホストのコンテナ上開発環境に直接アクセスする","date":"2022-12-17T00:00:00.000Z","ref":"/posts/2022/12/vscode-server-devcontainer","desc":"\n今回は「ぼくのかんがえたさいきょうのかいはつかんきょう」を紹介したいと思います。\n\nVS Code Server を使い、リモートサーバー上でコンテナとして動かしている開発環境に直接乗り込んでみよう、というアイデアです。\n\nSSH もポート開放も不要なのでとてもお手軽です。\n\n## 環境\n\nサーバー\n* Raspberry Pi 400 (Ubuntu 22.04.1, arm64)\n  * Docker & Docker Compose がインストールされていること\n\nクラ","draft":false,"content":"\n今回は「ぼくのかんがえたさいきょうのかいはつかんきょう」を紹介したいと思います。\n\n[VS Code Server](https://code.visualstudio.com/docs/remote/vscode-server) を使い、リモートサーバー上でコンテナとして動かしている開発環境に直接乗り込んでみよう、というアイデアです。\n\nSSH もポート開放も不要なのでとてもお手軽です。\n\n## 環境\n\nサーバー\n* Raspberry Pi 400 (Ubuntu 22.04.1, arm64)\n  * Docker & Docker Compose がインストールされていること\n\nクライアント\n* VS Code 1.74.1\n  * [Remote - Tunnels](https://marketplace.visualstudio.com/items?itemName=ms-vscode.remote-server) 拡張機能がインストールされていること\n\n払い出された URL にアクセスすればブラウザからでも使えるので、iPad などのモバイル端末でも使えますね。\n\n## ソースコード\n\nhttps://github.com/SogoKato/simple-devenv-py\n\n## アーキテクチャ\n\n![architecture](//www.plantuml.com/plantuml/png/dLAnJiCm4Dtz5QSmPu2vieegPb1JmNnoZjJ2EKU-Isc5-k-ene5q849ikPVttZq_UosAISS-64nkxtjKWfiTkJt74BiJLCyDR69B5Q202vvOORNIRqBTqi4xijQOH4wHkq1GRQcFIl3OLF1X06P_Df4LFLFwCfocJBiYbhtGK3eKjkJFGWNu9V1BJ6yoe2DuE2gn-CXPJKUzlOuk9r7gQucl-ew9hFstyTrVZCzcmRo9OtBqKxNaUKcnezHxnW1FAJeI8Sb2BV2IT3ioU-xWVXY2TwZJIN0Op2NdsPXorRKjhPjIVbtRacsEJ4jdM3PR4xUNj_K9)\n\n### なぜコンテナで動かすか？\n\n最近はホスト上に言語やライブラリをインストールするのではなく、コンテナを使って開発環境を整えることの方が多いと思います。\n\nもちろん仮想環境を使うという手段もありますが、可搬性ではコンテナの方が上なので、私は大抵の場合コンテナを使った開発環境を作り、VS Code の dev container を使ってコンテナ内で作業をします。\n\nしかしながら、今回導入する VS Code Server では2022年12月現在まだ dev container をはじめとするリモート開発の拡張機能はサポートされていません。\n\n> Can I use the Remote Development Extensions or a dev container with the VS Code Server?  \n> Not at this time.\n\nhttps://code.visualstudio.com/docs/remote/vscode-server#_can-i-use-the-remote-development-extensions-or-a-dev-container-with-the-vs-code-server\n\nなので、ホスト上に VS Code Server をインストールしたところで実際の開発環境に入り込むことができないので、であれば開発環境のコンテナ上に VS Code Server を入れて直接乗り込もう、というのが今回の趣旨になります。\n\n## セットアップ\n\n```sh\ngit clone https://github.com/SogoKato/simple-devenv-py.git\n```\n\nDockerfile の抜粋です。イメージのビルド時にスクリプトをダウンロード&実行して VS Code Server をインストールしています。\n\nCMD に `code-server` コマンドを書いて、コンテナ起動時に VS Code Server が実行されるようにします。\n\n`--accept-server-license-terms` と `--random-name` オプションはユーザーインタラクションを不要にするためのもので、順に[ライセンス](https://code.visualstudio.com/license/server)への同意とランダムな命名をしています。  \n`--server-data-dir` を使ってデータの永続化のため VS Code Server のデータの保存領域をバインドマウントするディレクトリ配下にしておきます。[^1]\n\n[^1]: ただし、筆者が一度コンテナを再作成してみたところ GitHub の再認証は不要でしたが、インストールした拡張機能などは消えてしまっていました（調査中）。\n\n```dockerfile\nRUN wget -O- https://aka.ms/install-vscode-server/setup.sh | sh\n\nCMD [\"code-server\", \"serve\", \"--accept-server-license-terms\", \"--random-name\", \"--server-data-dir\", \"/workspace/.vscode-server\"]\n```\n\nイメージをビルドしましょう。\n\n```sh\ndocker compose build\n```\n\ndocker-compose.yml です。なんの変哲もありません。\n\n```yml\nversion: '3'\nservices:\n  app:\n    build:\n      context: ./\n      dockerfile: Dockerfile\n    volumes:\n      - type: bind\n        source: ./\n        target: /workspace\n```\n\n起動します。\n\n```sh\ndocker compose up -d\n```\n\nログを見ると GitHub へのログインを求められています。\n\n```sh\ndocker compose logs -f\n```\n\n```\nsimple-devenv-py-app-1  | To grant access to the server, please log into https://github.com/login/device and use code xxxx-xxxx\n```\n\n言われた通り https://github.com/login/device にアクセスして、コードを入力しましょう。\n\nログインが完了するとトンネルが作成され、ブラウザアクセス用の URL が払い出されることがわかります。\n\n```\nsimple-devenv-py-app-1  | [2022-12-17 09:06:03] info Creating tunnel with the name: dazzling-antshrike\nsimple-devenv-py-app-1  | \nsimple-devenv-py-app-1  | Open this link in your browser https://insiders.vscode.dev/+ms-vscode.remote-server/dazzling-antshrike/workspace\n```\n\n## 接続（ブラウザ）\n\n好きなブラウザで払い出されたリンクを開くだけです。簡単。\n\n## 接続（VS Code）\n\n[Remote - Tunnels](https://marketplace.visualstudio.com/items?itemName=ms-vscode.remote-server) 拡張機能がインストールし、GitHub にログインすると登録されているトンネルの一覧を確認できます。\n\n![Tunnels](/images/posts/2022/12/vsc_remote_tunnels.png)\n\nボタンをクリックしてリモートに接続します。\n\n## 遊んでみる\n\nあとはいつも通り VS Code の設定を行なっていけば OK です。\n\n![editor](/images/posts/2022/12/vsc_remote_editor.png)\n\nサーバーを起動してみます。\n\n![run](/images/posts/2022/12/vsc_remote_run.png)\n\nポートが使われていることを検知するとローカル実行時と同じように通知が出てきます。  \n「ブラウザーで開く」をクリックするとポートがクラウド経由でポートが転送されて、起動したサーバーにリクエストが届きます。\n\n![port forwarding](/images/posts/2022/12/vsc_port_forwarding.png)\n\nちなみにこの URL は GitHub 未認証の状態では見ることができない（ログインを求められる）のでセキュリティ的にも安心です。\n\n## 使い心地は？\n\n若干ラグがあるように感じます。\n\n今回検証に使った Raspberry Pi のスペックの問題なのか（MicroSD から SSD にしたら変わるかも？）、ネットワーク環境の問題なのか、原因はまだ切り分けできていません。\n\nまた、ちょいちょい接続が切れて `Connecting to hogehoge...` と出てくるのですが、これはおそらくうちのネットワーク環境のせいな気がします。。\n\n### 2022/12/18 追記\n\nMicro SD から SSD に変えてみたら体感が大きく改善され、ストレスを感じない程度に快適になりました。SSD しか勝たん。\n\n## まとめ\n\n今までこれと同様のことを実現する選択肢として [code-server](https://coder.com/docs/code-server/latest) がありましたが、Pylance などオープンソースでは利用できない拡張機能があったりして、ローカルの VSCode と同じ環境を整えることは困難でした。\n\nMicrosoft 謹製の VS Code Server がリリースされたことにより、よりローカルの VS Code に近い環境を作ることができるようになりました。  \nサーバー側のポートを開ける必要がないのもメリットです。企業のポリシー次第ですが、会社でこれを使えたら嬉しいというユーザーも多いのではないでしょうか。\n\nそれでは、ハッピーなエンジニアライフを！\n\n## 参考文献\n\n* [Visual Studio Code Server](https://code.visualstudio.com/docs/remote/vscode-server)\n","tags":[{"name":"VS Code","ref":"/tags/vs-code"},{"name":"開発環境","ref":"/tags/開発環境"}]},{"title":"よくあるSPA+API構成でのOpenID Connectクライアント実装","date":"2022-12-02T00:00:00.000Z","ref":"/posts/2022/12/openid-connect-fastapi","desc":"\nこの記事はニフクラ等を提供している、富士通クラウドテクノロジーズ Advent Calendar 2022の2日目の記事です。\n\n昨日は @ntoofu さんの パケットキャプチャからKubernetes APIのTLS通信を解析する でした。  \n私は ","draft":false,"content":"\nこの記事は[ニフクラ](https://www.nifcloud.com/)等を提供している、[富士通クラウドテクノロジーズ Advent Calendar 2022](https://qiita.com/advent-calendar/2022/fjct)の2日目の記事です。\n\n昨日は [@ntoofu](https://qiita.com/ntoofu) さんの [パケットキャプチャからKubernetes APIのTLS通信を解析する](https://ntoofu.github.io/blog/post/sniffing-kube-apiserver-tls/) でした。  \n私は TLS な時点でパケットキャプチャを諦めてしまいそうですが Linux の便利な仕組みと気合があれば TLS 1.3 のパケットキャプチャも可能だとわかり、とても有益でした。私もギークなエンジニア目指して頑張ります。\n\n今日は OpenID Connect のクライアントをどう実装するかについて検討してみたいと思います。\n\nFastAPI + SPA (Vue.js) でちょっとした社内ツールを開発した時に社内の認可基盤との OpenID Connect を用いたログイン連携機能を作りたかったのですが、実装のための情報が少なかったので記事に残しておきたいと思ったのがきっかけです。「これがベストプラクティスだ！」というわけではありませんが、1つの実践例としてどなたかの参考になれば幸いです。\n\n## 対象読者\n\n* OpenID Connect を使ってログインするアプリを作りたいけど実装方法がわからない人\n  * 色んなフローがあるっぽいけどどれを使うべき？\n  * アクセストークン？ ID トークン？\n  * サーバーで何してクライアントで何するの？\n* Python の FastAPI で作ったサーバーでのログイン〜リダイレクト〜トークン検証の実装例が知りたい人\n\n## OpenID Connect とは\n\n**OpenID Connect は OAuth 2.0 を拡張する形で策定された、認証・認可のための仕組み**です。OAuth 2.0 は認可を行うことを目的とした仕様です。すでに10年も前の話ですが、OAuth を認証にも使ってしまう「SNS ログイン」の手法は「[単なる OAuth 2.0 を認証に使うと、車が通れるほどのどでかいセキュリティー・ホールができる](https://www.sakimura.org/2012/02/1487/)」と指摘されていました。OpenID Connect は OAuth 2.0 を拡張して認証の用途にも使えるようにした、ID トークンを発行するための仕組みとなっています。\n\n下図は認証・認可の流れの一例です（response_type=id_token の場合）。\n\n![overview](//www.plantuml.com/plantuml/png/NP6zIiDG5CVt-nINJkbGgBg9I0SNfrlo1g7DKD0q9EckzpW4KHfGS6aH9KWeIeMYK1FmOGv9u-GhUEvDH9lbSCBv_J_2xVc1vGMJqnDc3OAnnn6U43AKxpIvvVE9Rtjyx0rfxdIPI-neC78j9-0jb6yAXHkKQswO_NPB2Sn-ZUysSE7Qpl4HmXt22qA4CaOuuuQeTU9NjzTbEhHpgBpskSBbgyPN23EKdsgHIuG50j3222DOABXSN9T9HYS5o8IQ8OILtq67a8RVvZRzcZyYf7bqLLnCgy_o8Td47vMewPlIaa-NJEX8y__fMOVDTRcreVuqHCXqqLMRcN-2xLCHpqXUtrLces8HHldb_NTspdgsCwI7-W40)\n\nOP: OpenID Provider; ID トークンの発行者  \nRP: Relying Party; OP に認証機能を既存するクライアントアプリ\n\nユーザーがサービスにログインしようとすると、サービスはまず OpenID プロバイダー（OP）のエンドポイントに対してユーザーをリダイレクトします。OP は未認証であればログイン画面を出しユーザーを認証してから、サービスに対して認可を行うかどうか同意を取ります。ユーザーが同意すると、OP は ID トークンをリクエストに含めてサービスのコールバック URL にリダイレクトします。サービスは受け取った ID トークンが正規の OP からのものであることを OP の公開鍵を使用して検証します（そのほかにも複数の検証をする）。検証が正常に終了すればサービスへのログインの処理は完了です。\n\n## で、どうやってクライアントを実装するの？\n\n### まずは結論\n\n![code flow](//www.plantuml.com/plantuml/png/RPBFIW9H5CRtzoakhdGXJNzM4Q7GnfL3FS6aBeHI6KTewTmROQ4e94YWX28XKXdq1t8a7-OuEgvwXRwPKN2apULSlz_vpdUk4oiQccwKBY-ObZBoEYVvH792uWidrugyLCpeFA-dSUugx3n_nKCaFbr4tfFuvk5JDH9Y1NXaKzc2bZFucHfVDUmf0I6k9bR2li8okJI7Mm089GkPNEA4P8la2ya6YJx9CWydCSBDabHN_GSAyt95ZxrfXzpbnPl7lvDiavYwXHYH79AKA1Wuu6w6RTniaVb3vWE31WHJG3Z3cZEOkEqm4GDiIhBY3psA0jaoMJIjPQT7qh8RrVbrtRywtS6YF_QRjdqj57PznF2Zdsf3U_QcTM2B8ko39B0NjDj8C6PGN9RDsRGRC2NHyrQm_1N0uGhh7RppnjMPDktQX--zRWqIytuRwLpY_sUtNwkpyStwdRsbWy2yqh3l7dyd9elXpySNzmS0)\n\n* API サーバーで認証リクエストや ID トークン検証を行う\n  * 今回は採用するライブラリの都合で Authorization Code Flow を使います\n* ID トークンは Cookie で管理\n  * localStorage ではセキュリティ上問題がある\n  * インメモリでは利便性に欠ける\n  * Cookie の属性\n    * HttpOnly: true\n      * JavaScript からは触らせない\n    * SameSite: Lax\n      * 他サイトへの Cookie 送信を最低限にする\n      * Strict にすると OP にリダイレクトした際に破棄されてしまうため Lax\n    * Secure: true\n      * HTTPS でのみ Cookie を送信する\n\n### SPA なら全部 SPA にやらせればいいんじゃないの？\n\nはい、Implicit Flow を使うことで実現できます。\n\n冒頭の「車が通れるほどのどでかいセキュリティー・ホール」の話は OAuth 2.0 を Implicit Flow で使用した時の話ですが、OpenID Connect はそれを踏まえて策定された仕様なので、OpenID Connect において Implicit Flow を使うことに問題はありません。\n\nImplicit Flow を使う場合、下図のようになります（response_type=id_token）。\n\n![implicit flow](//www.plantuml.com/plantuml/png/TP9FIm916CRlyoa6JteGjZydYL3euicbFi6c7eHIMLVegFD6I1WA1LsKC2H4AWCfq5tmmxpkkfxw2hqpxWO3THbcz_dDypppxcORZcKxpSiBPXMTciqHNX0y55-qSgl1cusopMjsYTOzWvtNhdW2nQT4u1x5WYTFpLI2rScZKgpKhQh3pynST63Vq8IScO-40uELgoLERXgGADJBrVm9mYF26q8VnHYXnPC5Yf1T2cPq_j1WgbVwMALbkEJ5X-Bd20CKAxaHCuGf0j264KSuMH0TJk_2YKUQ9CI4he7GsJaUfIMY6suUtEtm6S7r-ztWkhTx34UJpNWPrz1zNThulHcZbxyDO-rLfGrLlKLINhQZvarLvwcufPnKXklYjjLUhqQCfF-8O3oW24dyFHZ_lRjUtiGPghaE19s-V_lqxRLPbZuF_HC_)\n\n上記のフローでは、SPA 側で Client Secret を管理する必要がありません。ClientSecret が必要になるのは主にトークンエンドポイント（認可コードやアクセストークンを使って、アクセストークンや ID トークンを取得するためのやつ）を使う時なので、response_type=id_token で OIDC クライアントを実装する場合には困りません。response_type=id_token で問題がないかという観点では、ログイン完了時にだけユーザー情報が手に入れば問題ない場合（ユーザー情報が最新である必要がない場合）には事足りると思われます。\n\nSPA でエンドユーザーに見せたくない機密情報を扱うのはほとんど不可能だと思いますので Client Secret を SPA に持たせる必要がないのは嬉しい仕様ですね。**Implicit Flow では CSRF 対策のために nonce の検証が必須**なので注意してください。\n\n### ID トークンどこ置く問題\n\nただ、Client Secret を SPA で管理する必要がないからといって SPA 上で ID トークンを扱えるようにすると、ID トークンをどこに保管するか、という問題が出てきます。これは OIDC のスコープ外の議論ですが、クライアント実装にあたって必ず検討するべき点だと思うので、今回はこれも考えていきたいと思います。\n\nID トークンを置く場所として考えられる候補を比較してみます。\n\n||Cookie (HttpOnly: true, Secure: true, SameSite: Lax)|インメモリ|localStorage|\n|---|---|---|---|\n|保持期間|**設定された有効期限まで**|ページがリロードされるまで|**なし**|\n|CSRF 対策|**更新系は防げる**|**他の JS ライブラリは基本的にアクセスできない**|どの JS ライブラリも取得可|\n|JS での ユーザー情報取得|できない [^1]|**できる**|**できる**|\n\n[^1]: サーバー側で Cookie の ID トークンを検証した上で、JSON レスポンスでユーザー情報を返却する API を作ったりすれば可能です。\n\nどれも一長一短な感がありますが、この中で一番安全性と利便性のバランスがいいのは Cookie (HttpOnly: true, Secure: true, SameSite: Lax) だと思ったので、今回は Cookie に ID トークンを置くようにしました。\n\nなお、Auth0 のクライアントライブラリではインメモリに ID トークンを保存しつつ、セッションを長く保たせることもできるようです。  \n[SPA認証トークンはlocalStorageでもCookieでもない、Auth0方式はいいねというお話](https://mizumotok.hatenablog.jp/entry/2021/08/04/114431)\n\n## FastAPI で OpenID Connect クライアントを実装する\n\n以上で、一通りクライアントの実装方針について議論ができたと思うので、ここからは具体的な Python (FastAPI) での実装に移りたいと思います。  \n今回使うライブラリ Authlib では FastAPI の他に Starlette, Flask や Django の Oauth Client とその実装例も公開されているので、それらを参考にすれば他のフレームワークでも割と簡単に実装できるのではないかなと思います。\n\n### ソースコード\n\nhttps://github.com/SogoKato/oidc-fastapi-authlib\n\n動かしてみたい方は README に従って起動してみてください。\n\n### 必要なもの\n\n前準備として OpenID プロバイダを用意する必要があります。どのプロバイダでも大丈夫ですが、まだ持っていない場合は [Auth0](https://auth0.com/jp) に登録してみるのがおすすめです。個人で使うようなリクエスト量であれば無料で使えます（2022年12月現在）。\n\n登録後、Application を作成したら、リダイレクト URI (Allowed Callback URLs) に `http://localhost:8080/api/auth` を入れておきます。また、下記の情報を探してメモっておきましょう。\n\n* Client ID\n* Client Secret\n* OpenID Configuration Endpoint\n  * アクセスすると OIDC クライアントで必要な情報を返してくれるエンドポイント\n  * 通常 `https://example.com/.well-known/openid-configuration`\n\n### アーキテクチャ\n\n![architecture](//www.plantuml.com/plantuml/png/SoWkIImgAStDuKfCBialKdZSlEnnyvx7JTk0f49YiK9fSMeHLs9HSaPcRc99ge9oIMfoHbv-JdvwfK9UUcPUXOAD3L15MMPogfqT3dME0Pv4g5Bo2F7rqNSE3jRt2bO2sGnqM4bcCefEa6CKTEsWDbi17RlgSTFwnqqh7ZVjVDpSmGKHrzMr0za9bDTFBCX4249D18bpEQJcfG0z3G00)\n\n今回は Cookie の SameSite 属性を使用しているので、SPA と API とで同じドメイン名を使い、パスでリクエストを振り分けます。\n\n### 解説\n\n#### ログイン時の処理\n\n```python\napp = FastAPI()\napp.add_middleware(SessionMiddleware, secret_key=\"MYSTRONGKEY\", https_only=True)\n```\n\nFastAPI の初期化と Cookie のための SessionMiddleware の追加をします。\n\n```python\noauth = OAuth()\noauth.register(\n    name=\"auth0\",\n    server_metadata_url=\"https://example.com/.well-known/openid-configuration\",\n    client_id=\"クライアントID\",\n    client_secret=\"クライアントシークレット\",\n    client_kwargs={\"scope\": \"openid profile\"},\n)\n```\n\nAuthlib のインスタンスを作ります。scope に `openid` と入れておくことで Authorization Code Flow の時にトークンエンドポイントで ID トークンが手に入ります。\n\n```python\n@app.get(\"/api/login\")\nasync def login(request: Request):\n    redirect_uri = request.url_for(\"auth\")\n    return await oauth.auth0.authorize_redirect(request, redirect_uri)\n```\n\n認証リクエストを開始するためのエンドポイントです。ユーザーがここにアクセスすることで `http://localhost:8080/api/auth` をリダイレクト URI とした認証リクエストを開始します（OpenID プロバイダにリダイレクトされる）。\n\nちなみにこんな URL でリダイレクトされます。state と nonce があることも確認できますね。\n\n```\nhttps://example.com/authorize\n  ?response_type=code\n  &client_id=クライアントID\n  &redirect_uri=http%3A%2F%2Flocalhost%3A8080%2Fapi%2Fauth\n  &scope=openid+profile\n  &state=PGO5TxTujESoXuLlfzYTWZsioK5Up5\n  &nonce=HfyA3eugosoOuieiTGRZ\n```\n\n![OP login](/images/posts/2022/12/oidc_op_login.png)\n\nログインが完了するとリダイレクト URI にリダイレクトされ、次のエンドポイントが呼ばれます。\n\n```python\n@app.get(\"/api/auth\")\nasync def auth(request: Request):\n    try:\n        token = await oauth.auth0.authorize_access_token(request)\n    except OAuthError:\n        logger.exception(\"An error occurred while verifying authorization response.\")\n        raise UnauthenticatedError()\n    userinfo = token.get(\"userinfo\")\n    # userinfoのclaims(subやnameなど)を使ってDBにユーザーを登録する処理がここにきます.\n    request.session[\"id_token\"] = token.get(\"id_token\")\n    return RedirectResponse(url=\"/\")\n```\n\n`authorize_access_token` で認可コードと state を使って ID トークンを取得し、ID トークンと nonce を検証するところまでやってくれます（楽ちん）。  \nその後は自分の好きなように処理をして OK です。sub クレームをユーザー ID として、ユーザーがまだ DB に登録されていなければ insert するとか、ユーザーのプロフィール情報が変わってたら更新するとか、そういう処理が来るのかなと思います。\n\n最後に、Cookie に ID トークンをセットして `/` にリダイレクトして、ログイン処理は完了です。\n\n#### ログイン後の処理\n\nログイン後は JS 側で fetch や axios でリクエストをすると、Cookie も自動的に送信されます。なので、ログインしたユーザーにしか使わせたくないエンドポイントでは、Depends を使って ID トークンを検証します。\n\n```python\n@app.get(\"/api/items\")\nasync def list_items(user: User = Depends(verify_user)):\n    logger.info(f\"Successful log in: user_id={user.id} name={user.name}\")\n    return {\n        \"items\": [\n            {\"name\": \"Teddy bear\", \"icon\": \"🧸\", \"price\": 99},\n            {\"name\": \"Apple\", \"icon\": \"🍎\", \"price\": 2},\n            {\"name\": \"Sushi\", \"icon\": \"🍣\", \"price\": 200},\n            {\"name\": \"Bento\", \"icon\": \"🍱\", \"price\": 50},\n        ]\n    }\n```\n\n`verify_user` 関数で Cookie から ID トークンを取り出します。\n\n```python\nasync def verify_user(request: Request):\n    id_token = request.session.get(\"id_token\")\n    if id_token is None:\n        raise UnauthenticatedError()\n    decoded_jwt = await verify_token(id_token=id_token)\n    # DBにユーザーが登録されているか確認する処理がここにきます.\n    # user = user_repo.select_by_user_id(user_id=user_id)\n    return user\n```\n\n`verify_token` が ID トークンを検証する関数です。\n\n```python\nasync def verify_token(id_token: str):\n    jwks = await oauth.auth0.fetch_jwk_set()\n    try:\n        decoded_jwt = jwt.decode(s=id_token, key=jwks)\n    except Exception:\n        logger.exception(\"An error occurred while decoding jwt.\")\n        raise UnauthenticatedError()\n    metadata = await oauth.auth0.load_server_metadata()\n    if decoded_jwt[\"iss\"] != metadata[\"issuer\"]:\n        raise UnauthenticatedError()\n    if decoded_jwt[\"aud\"] != settings.oidc_client_id:\n        raise UnauthenticatedError()\n    exp = datetime.fromtimestamp(decoded_jwt[\"exp\"])\n    if exp < datetime.now():\n        raise UnauthenticatedError()\n    return decoded_jwt\n```\n\nID トークンの検証として最低限必要なのは以下の通りです（Authorization Code Flow の場合）。\n\n1. JWK Set（OP の公開鍵）を使用して JWT をデコードする\n2. iss クレーム（Issuer Identifier; 発行者）を検証する\n3. aud クレーム（Audience(s); 誰に対して発行したか = Client ID）を検証する\n4. exp クレーム（Expiration time; 有効期限）を検証する\n\n以上が完了すれば基本的な OIDC クライアント実装は完了です🎉\n\n![log in](/images/posts/2022/12/oidc_log_in.gif)\n\n## 最後に\n\nいかがでしたか？？\n\n一見複雑そうな OpenID Connect ですが、一つずつ紐解いてみると意外と簡単に実装できるように仕様が設計されていることがわかりました。自分でパスワードを頑張って管理するよりもこういうところは信頼できる OpenID プロバイダに任せてしまった方が楽ですし、何よりも安全ですよね。\n\nぜひ皆さんも Web サービスを作る時には活用してみてください。\n\nこの記事は[富士通クラウドテクノロジーズ Advent Calendar 2022](https://qiita.com/advent-calendar/2022/fjct)の2日目の記事でした。\n\n明日は [@Syuparn](https://qiita.com/Syuparn) さんが SQL のテストについて書いてくれるようです。  \nSQL のテストってあまりやってなかったりするので、他の人がどのように考えて実施しているのか気になります。それでは、明日の記事もお楽しみに！\n\n（👇この記事がよかったらいいねボタンを押してください！）\n\n## 参考文献\n\n* [OpenID Connect Basic Client Implementer's Guide 1.0 - draft 42](https://openid.net/specs/openid-connect-basic-1_0.html)\n* [OpenID Connect Implicit Client Implementer's Guide 1.0 - draft 25](https://openid.net/specs/openid-connect-implicit-1_0.html)\n* [Google login for FastAPI](https://blog.authlib.org/2020/fastapi-google-login)\n* [一番分かりやすい OpenID Connect の説明](https://qiita.com/TakahikoKawasaki/items/498ca08bbfcc341691fe)\n* [IDトークンが分かれば OpenID Connect が分かる](https://qiita.com/TakahikoKawasaki/items/8f0e422c7edd2d220e06)\n* [OpenID Connect 全フロー解説](https://qiita.com/TakahikoKawasaki/items/4ee9b55db9f7ef352b47)\n* [OAuth 2.0/OpenID Connectの2つのトークンの使いみち](https://qiita.com/wadahiro/items/ad36c7932c6627149873)\n* [単なる OAuth 2.0 を認証に使うと、車が通れるほどのどでかいセキュリティー・ホールができる](https://www.sakimura.org/2012/02/1487/)\n* [OIDCのImplicit FlowでClientSecretを使わずにID連携する](https://zenn.dev/ritou/articles/a)\n* [SPA認証トークンはlocalStorageでもCookieでもない、Auth0方式はいいねというお話](https://mizumotok.hatenablog.jp/entry/2021/08/04/114431)\n* [GoでOpenID ConnectのClientを実装する（実装編）](https://times.hrbrain.co.jp/entry/go-openid-connect-implement)\n","tags":[{"name":"OpenID Connect","ref":"/tags/openid-connect"},{"name":"認証/認可","ref":"/tags/認証-認可"},{"name":"SPA","ref":"/tags/spa"},{"name":"Python","ref":"/tags/python"},{"name":"FastAPI","ref":"/tags/fastapi"}]},{"title":"GitLab CIのrulesとworkflowを理解する","date":"2022-11-17T00:00:00.000Z","ref":"/posts/2022/11/gitlab-rules-workflow","desc":"\nGitLab CI の rules を使って Dockerfile などの特定のファイルの変更時のみ Docker イメージを作成するパイプラインを回して、それ以外の時には既存の Docker イメージを使用して CI を実行する、という組み方をしたかったのですが、書き方に結構手間取ったのでメモ。\n\n環境: GitLab.com 15.6.0-pre\n\n## rules とは\n\nhttps://docs.gitlab.com/ee/ci/yaml/#rules\n\nそれぞれのジョブについて、パイプラインに追加するかしないかの条件を記述するものです。\n\nrules では下記の条件が指定できます。","draft":false,"content":"\nGitLab CI の rules を使って Dockerfile などの特定のファイルの変更時のみ Docker イメージを作成するパイプラインを回して、それ以外の時には既存の Docker イメージを使用して CI を実行する、という組み方をしたかったのですが、書き方に結構手間取ったのでメモ。\n\n環境: GitLab.com 15.6.0-pre\n\n## rules とは\n\nhttps://docs.gitlab.com/ee/ci/yaml/#rules\n\nそれぞれのジョブについて、パイプラインに追加するかしないかの条件を記述するものです。\n\nrules では下記の条件が指定できます。\n\n* `if`\n* `changes`\n* `exists`\n* `allow_failure`\n* `variables`\n* `when`\n\nそれぞれの条件の詳細については公式ドキュメントを参照してください。\n\nrules は [only/except](https://docs.gitlab.com/ee/ci/yaml/#only--except) を置き換えるものなので、rules と only/except を同じジョブで同時に指定することはできません。\n\nrules の指定の一例を公式ドキュメントから引用します。\n\n```yaml\ndocker build:\n  script: docker build -t my-image:$CI_COMMIT_REF_SLUG .\n  rules:\n    - if: $CI_PIPELINE_SOURCE == \"merge_request_event\"\n      changes:\n        - Dockerfile\n      when: manual\n      allow_failure: true\n```\n\n上記の例では\n\n* パイプラインがマージリクエストのパイプラインの時に `Dockerfile` が変更されているか確認する\n* `Dockerfile` が変更されている時に、ジョブをパイプラインにマニュアルジョブとして追加する\n  * `allow_failure: true` によって、ジョブがトリガーされなかったとしても後続のジョブが実行される\n* `Dockerfile` が変更されていない時は、ジョブをパイプラインに追加しない\n  * `when: never` と同じ\n\nという挙動になります。\n\nrules はリストなので複数のルールを書くことができますが、 **短絡評価である** 点に注意が必要です。rules はパイプラインが作成されたタイミングで評価され、最初にマッチするまで評価が行われます。そのため、例えば `- when: manual` を最初に記述するとそこで評価が終わり（`manual` は常に真となりパイプラインに追加されます）、その後の条件については評価されません。言われてみたらそれはそうなのですが、筆者はそこでしばらく詰まってました。\n\n## workflow とは\n\nhttps://docs.gitlab.com/ee/ci/yaml/workflow.html\n\nパイプラインそのものを実行するかどうかを決定するものです。workflow で条件にマッチしなかった場合、そのパイプライン内のジョブが実行されることはありません。\n\nよくある使い方としては `$CI_PIPELINE_SOURCE` の種類（`merge_request_event`, `push`, `schedule`, etc.）に応じてパイプラインを実行するかしないかを決めると言った使い方があります。\n\n```yaml\nworkflow:\n  rules:\n    - if: $CI_PIPELINE_SOURCE == \"schedule\"\n      when: never\n    - if: $CI_PIPELINE_SOURCE == \"push\"\n      when: never\n    - when: always\n```\n\n上記の例ではスケジュール実行時または push 時（ブランチとタグ）には `when: never` が指定されているためパイプラインは走りません。それ以外の時にはパイプラインが実行されます。\n\n## 重複したパイプラインを避ける\n\nhttps://docs.gitlab.com/ee/ci/jobs/job_control.html#avoid-duplicate-pipelines\n\nジョブに rules を使用していると、マージリクエスト作成後にブランチに対して push するといった1つのアクションが、push 時に発生したパイプラインとマージリクエストのパイプラインの2つが走らせることが起こりえます。\n\n重複したパイプライン（duplicate pipelines）を避けるためには\n\n* workflow を使ってどの種類のパイプラインは走って良いのかを指定する\n* ジョブが実行される条件をかなり限定的にして、最後のルールとして `when`（`when: never` 以外）を使うのを避ける\n\nことが対策になります。\n\n筆者の場合は、次項で示す例を書いている際に重複したパイプラインが発生し、片方のジョブは正しく機能しないという状況が起こりましたが、workflow を指定することで重複が解消され、正しく機能するようになりました。\n\n## 特定ファイルの変更時のみジョブを実行し、それ以外はスキップして後続のジョブを実行する\n\n`.gitlab-ci.yml` と同階層に `Dockerfile` がある想定です。\n\n```yaml\nstages:\n  - build\n  - test\n\nworkflow:\n  rules:\n    - if: $CI_PIPELINE_SOURCE == \"merge_request_event\"\n    - if: $CI_COMMIT_BRANCH && $CI_OPEN_MERGE_REQUESTS\n      when: never\n    - if: $CI_COMMIT_BRANCH\n\nbuild:\n  stage: build\n  image:\n    name: gcr.io/kaniko-project/executor:v1.9.0-debug\n    entrypoint: [\"\"]\n  script:\n    - /kaniko/executor\n      --context \"${CI_PROJECT_DIR}\"\n      --dockerfile \"${CI_PROJECT_DIR}/Dockerfile\"\n      --destination \"${CI_REGISTRY_IMAGE}:latest\"\n  rules:\n    - if: $CI_PIPELINE_SOURCE =~ /merge_request_event|push/\n      changes:\n        - Dockerfile\n    - when: manual\n      allow_failure: true\n\ntest:\n  stage: test\n  image: \"${CI_REGISTRY_IMAGE}:latest\"\n  script:\n    - echo \"DO SOME TESTS\"\n```\n\n上記の例は以下の挙動をします。\n\n* workflow\n  * マージリクエストでのパイプラインの場合は実行される\n  * マージリクエストがある時、ブランチへの push イベントでのパイプラインは実行されない\n  * それ以外のブランチへの push イベントでは実行される\n* rules\n  * パイプラインがマージリクエストのパイプラインの時または push された時に `Dockerfile` が変更されているか確認する\n    * changes は上記以外では常に真となってしまうため\n  * `Dockerfile` が変更されている時に、ジョブをパイプラインにジョブを追加する\n  * 上記の条件に当てはまらない時はマニュアルジョブとしてパイプラインに追加する\n    * `allow_failure: true` となっているので、後続のジョブはそのまま実行される\n\nこのように書くことで、実現したかった「特定ファイルの変更時のみジョブを実行し、それ以外はスキップして後続のジョブを実行する」が実現します。\n","tags":[{"name":"Gitlab","ref":"/tags/gitlab"},{"name":"CI/CD","ref":"/tags/ci-cd"}]},{"title":"Next.jsとTailwind CSSでブログを作るときに考えたこと","date":"2022-11-13T00:00:00.000Z","ref":"/posts/2022/11/blog-with-nextjs-and-tailwindcss","desc":"\nこのブログは Next.js の SSG（Static Site Generation; 静的サイト生成）機能を使いながら、デザインの大半は Tailwind CSS を使用して整えています。そして生成された HTML, CSS, JS は GitHub Pages でホストさせてもらっています。\n\nそこそこの出来栄えになったので、今回はこのブログができるまでのお話をしたいなと思ったのですが、正直なところ、以下のリンク先のページを~~まるパク~~参考にさせてもらいながら作成したので、具体的な構築方法についてはそちらをご覧いただけると良いかと思います。1ステップずつ丁寧に解説されておりとても有","draft":false,"content":"\nこのブログは Next.js の SSG（Static Site Generation; 静的サイト生成）機能を使いながら、デザインの大半は Tailwind CSS を使用して整えています。そして生成された HTML, CSS, JS は GitHub Pages でホストさせてもらっています。\n\nそこそこの出来栄えになったので、今回はこのブログができるまでのお話をしたいなと思ったのですが、正直なところ、以下のリンク先のページを~~まるパク~~参考にさせてもらいながら作成したので、具体的な構築方法についてはそちらをご覧いただけると良いかと思います。1ステップずつ丁寧に解説されておりとても有用でした🙏\n\n* [Next.jsを利用した初めての本格的Markdownブログサイトの構築](https://reffect.co.jp/react/nextjs-markdown-blog)\n\nなので、今回は技術的な詳細というよりも、筆者が始める前に疑問だった点や技術選定、設計まわりについて書ければなと思います。\n\n## 対象読者\n\n* 自分のブログを作ってみたい人\n* React や Next.js に興味がある人\n\n## ブログをどこでホストするか\n\n技術に関する記事を書くのであれば Qiita や Zenn でいいじゃんと思いますし、実際その方がはるかに楽で、多くのリアクションをもらいやすいと思います。そんな中、自分のブログを作る理由としては「やってみたかったから」以上の理由は存在しません。つまりロマンです。\n\nしかしながら、いざ自分のブログを作ろうと思ったときに\n\n* 簡単に、かつ高度なカスタマイズができる\n* ランニングコストが安い\n\nといった美味しい環境は思ったよりも少ないです。\n\nブログを作ると言ったら WordPress が超定番ですが、テーマを作るとなると php や WordPress の知識が必要になります。WordPress テーマづくりに携わったことがあるので、やってみると思ったより簡単なのですが、筆者のお仕事の分野とはかすらないのであまりモチベーションが湧きません。  \nまた、最低月数百円のランニングコストもかかりますし、動的にページを生成するのでレスポンスも遅くなりがちです。\n\n次に SSG（Static Site Generation; 静的サイト生成）を検討します。Markdown で原稿を書き、静的な HTML などのファイルを出力すれば、GitHub Pages で無料で公開できるので結構良さそうです。\n\nSSG のツールとしては有名なものがいくつかあります。\n\n* Next.js\n* Gatsby\n* Hugo\n* NuxtJS\n* Jekyll\n\nこのうち、Next.js と Gatsby は React ベースのため当初は検討から外していました。筆者は Hugo を選び、配布テーマを適用してみたり、自作テーマを作り始めたりしましたが、想像より学習コストが高かったので挫折してしまいました。\n\n代わりに最近波に乗っていそうな Next.js を使ってみることにしました。React を使うのは初めてだったので（チュートリアルしかやったことがない）躊躇していましたが、[最初に紹介した記事](https://reffect.co.jp/react/nextjs-markdown-blog)のおかげもあって、すんなりと構築することができました。React の基本的な知識さえあれば問題なさそうです。\n\n## どうやってデザインするか\n\nまずは Adobe XD でデザインカンプを作成します。いきなりマークアップを始めても、作りたいものが定まっていないと無駄に時間がかかってしまうので、多少手間でも作りたいもののイメージを先に決めておくと良いです。\n\n![XD](/images/posts/2022/11/xd.png)\n\n上図のような感じで、ヘッダーやサイドバー、記事一覧、記事ページをざっくりと作りました。\n\n## React の CSS よくわからん問題\n\n筆者の経験不足のせいなのですが、React で CSS でスタイルを適用するベストな方法がよくわかりませんでした😇\n\nデザインカンプを作ってみて、そこまで複雑な CSS を記述する必要がなさそうだったので、Tailwind CSS を使ってみることにしました。\n\n[Tailwind CSS](https://tailwindcss.com/) とは、HTML のクラス属性に `flex`, `pt-4`, `text-center` のようなクラス名を記述することで、`display: flex` をかけたり `padding-top` や `text-align: center` を設定できるというヤツです。\n\n軽く個人的な感想をまとめてみると\n\nPros\n* コード量が減る\n* HTML 要素を消したけど CSS は消し忘れた、みたいなことはなくなる\n* カスタムの色を設定できるなど、一定の柔軟性がある\n\nCons\n* CSS の知識は必要（それはそう）\n* 都度リファレンスを見てクラス名を確認する必要がある\n* 複雑なことはできないので割り切るか、別の方法で書かなくてはいけない\n\nな感じです。良いところも悪いところもありますが、今回筆者はアリだと判断して採用しましたし、実際良かったです。\n\n## クライアント側で実行させたい処理どう書くの\n\nSSG で静的なページを書き出すと言っても、クライアント側で実行させたい処理はあります。このサイトでは、ライトモード↔︎ダークモード切り替えや「いいね」ボタンがそれに該当します。いずれもユーザーのアクションによって DOM を書き換える必要性があります。\n\n普通に React のコンポーネントとして書いてしまうと SSG でのビルド時に静的なページとして書き出されてしまうため、なんとかする必要があります。最初は public ディレクトリ内に js ファイルを置いて Next.js の [Script](https://nextjs.org/docs/basic-features/script) タグで読み込ませていたのですが、それよりも Next.js の [Dynamic Import](https://nextjs.org/docs/advanced-features/dynamic-import) 機能を使った方がスマートに書けます。\n\n`{ ssr: false }` 引数を渡してあげることで、そのコンポーネントはブラウザ上でレンダリングされるようになります。\n\n## GitHub Pages で公開するときの罠\n\n晴れて準備完了！いざ公開！と意気揚々と GitHub Pages にデプロイしても、うまくいかないことがあります。\n\nビルド時には以下のコマンドを実行しましょう。\n\n```\nnext build\nnext export -o docs/\ntouch docs/.nojekyll\necho 'sogo.dev' > docs/CNAME\n```\n\nポイントは公開ディレクトリ（上記の場合は `docs`）の直下に `.nojekyll` というファイルを作成していることです。これがないと `_next` ディレクトリは以下が公開されずリンク切れになります。  \n参考: [Next.js の SSG 機能で生成した静的サイトを GitHub Actions 経由で GitHub Pages に公開する](https://sidearrow.github.io/article/next-js-ssg-on-github-pages)\n\n最後のコマンドはカスタムドメイン名を使用しない場合は不要ですが、使用する場合は都度生成しておかないと消えてしまうため、カスタムドメイン名でアクセスできなくなります。\n\n## 今後やっていきたいこと\n\n以上がこのブログを作るにあたって考えたことなのですが、まだやり残したことはあります。\n\n* 記事をリッチにしたい\n  * シンタックスハイライト\n  * リンクカード\n  * 目次（ToC）\n* おすすめ記事をいい感じのアルゴリズムで出したい\n  * まずは記事を書きためなきゃ。。\n\n## 最後に\n\nつらつらと駄文を書き連ねてしまいました。勘の良い方は気づいていると思いますが、このブログは GitHub Pages でホストされている＝[ソースが見れる](https://github.com/SogoKato/sogokato.github.io)なので、もし興味のある方がいらっしゃいましたら覗いてみてください。\n","tags":[{"name":"JavaScript","ref":"/tags/javascript"},{"name":"React","ref":"/tags/react"},{"name":"Next.js","ref":"/tags/next.js"},{"name":"Tailwind CSS","ref":"/tags/tailwind-css"}]},{"title":"Hello World!","date":"2022-10-11T00:00:00.000Z","ref":"/posts/2022/10/hello-world","desc":"\nはじめまして。  \nこれは初めての投稿です。\n\n今までブログが長く続いたことがないのですが、n度目の正直ということで今回こそは長く続くように頑張りたいと思います（とても固い決意）。\n\n@SogoKato といいます。どんな人か気になってくれた方は 自己紹介ページ をご覧いただければと思います。\n\nこのブログの制作にあたっては、初めて React + Next.js + Tailwind CSS を触って作ってみましたが、結構いい開発者体験だったのでこれについてもまた記事に起こしていきたいな〜と思っています。。（少しゆるい決意）\n\n今まで書いてきた記事については、[私の","draft":false,"content":"\nはじめまして。  \nこれは初めての投稿です。\n\n今までブログが長く続いたことがないのですが、n度目の正直ということで今回こそは長く続くように頑張りたいと思います（とても固い決意）。\n\n@SogoKato といいます。どんな人か気になってくれた方は [自己紹介ページ](/profile) をご覧いただければと思います。\n\nこのブログの制作にあたっては、初めて React + Next.js + Tailwind CSS を触って作ってみましたが、結構いい開発者体験だったのでこれについてもまた記事に起こしていきたいな〜と思っています。。（少しゆるい決意）\n\n今まで書いてきた記事については、[私の Qiita](https://qiita.com/SogoK) を見てみてください。  \nおすすめは↓らへんです。\n\n* [Vue 3から始める人のための学習ロードマップ](https://qiita.com/SogoK/items/15ed0d9b2be4279b2f47)\n* [新卒エンジニアがCKA取得を目指してKubernetesを勉強したときの記録](https://qiita.com/SogoK/items/4ed2e118d0412c868169)\n* [GitLabのタブを開きすぎて見分けづらいのでfaviconを変える拡張機能を作った](https://qiita.com/SogoK/items/31f74b517dc3c6884c04)\n\nさて、初回から頑張りすぎると次回以降の心理的なハードルがあがっちゃうのでこれくらいにしておこうと思います。では、これからよろしくお願いします🙏\n","tags":[{"name":"Personal","ref":"/tags/personal"}]}],"post":{"title":"GitLab CIのrulesとworkflowを理解する","date":"2022-11-17T00:00:00.000Z","ref":"/posts/2022/11/gitlab-rules-workflow","desc":"\nGitLab CI の rules を使って Dockerfile などの特定のファイルの変更時のみ Docker イメージを作成するパイプラインを回して、それ以外の時には既存の Docker イメージを使用して CI を実行する、という組み方をしたかったのですが、書き方に結構手間取ったのでメモ。\n\n環境: GitLab.com 15.6.0-pre\n\n## rules とは\n\nhttps://docs.gitlab.com/ee/ci/yaml/#rules\n\nそれぞれのジョブについて、パイプラインに追加するかしないかの条件を記述するものです。\n\nrules では下記の条件が指定できます。","draft":false,"content":"\nGitLab CI の rules を使って Dockerfile などの特定のファイルの変更時のみ Docker イメージを作成するパイプラインを回して、それ以外の時には既存の Docker イメージを使用して CI を実行する、という組み方をしたかったのですが、書き方に結構手間取ったのでメモ。\n\n環境: GitLab.com 15.6.0-pre\n\n## rules とは\n\nhttps://docs.gitlab.com/ee/ci/yaml/#rules\n\nそれぞれのジョブについて、パイプラインに追加するかしないかの条件を記述するものです。\n\nrules では下記の条件が指定できます。\n\n* `if`\n* `changes`\n* `exists`\n* `allow_failure`\n* `variables`\n* `when`\n\nそれぞれの条件の詳細については公式ドキュメントを参照してください。\n\nrules は [only/except](https://docs.gitlab.com/ee/ci/yaml/#only--except) を置き換えるものなので、rules と only/except を同じジョブで同時に指定することはできません。\n\nrules の指定の一例を公式ドキュメントから引用します。\n\n```yaml\ndocker build:\n  script: docker build -t my-image:$CI_COMMIT_REF_SLUG .\n  rules:\n    - if: $CI_PIPELINE_SOURCE == \"merge_request_event\"\n      changes:\n        - Dockerfile\n      when: manual\n      allow_failure: true\n```\n\n上記の例では\n\n* パイプラインがマージリクエストのパイプラインの時に `Dockerfile` が変更されているか確認する\n* `Dockerfile` が変更されている時に、ジョブをパイプラインにマニュアルジョブとして追加する\n  * `allow_failure: true` によって、ジョブがトリガーされなかったとしても後続のジョブが実行される\n* `Dockerfile` が変更されていない時は、ジョブをパイプラインに追加しない\n  * `when: never` と同じ\n\nという挙動になります。\n\nrules はリストなので複数のルールを書くことができますが、 **短絡評価である** 点に注意が必要です。rules はパイプラインが作成されたタイミングで評価され、最初にマッチするまで評価が行われます。そのため、例えば `- when: manual` を最初に記述するとそこで評価が終わり（`manual` は常に真となりパイプラインに追加されます）、その後の条件については評価されません。言われてみたらそれはそうなのですが、筆者はそこでしばらく詰まってました。\n\n## workflow とは\n\nhttps://docs.gitlab.com/ee/ci/yaml/workflow.html\n\nパイプラインそのものを実行するかどうかを決定するものです。workflow で条件にマッチしなかった場合、そのパイプライン内のジョブが実行されることはありません。\n\nよくある使い方としては `$CI_PIPELINE_SOURCE` の種類（`merge_request_event`, `push`, `schedule`, etc.）に応じてパイプラインを実行するかしないかを決めると言った使い方があります。\n\n```yaml\nworkflow:\n  rules:\n    - if: $CI_PIPELINE_SOURCE == \"schedule\"\n      when: never\n    - if: $CI_PIPELINE_SOURCE == \"push\"\n      when: never\n    - when: always\n```\n\n上記の例ではスケジュール実行時または push 時（ブランチとタグ）には `when: never` が指定されているためパイプラインは走りません。それ以外の時にはパイプラインが実行されます。\n\n## 重複したパイプラインを避ける\n\nhttps://docs.gitlab.com/ee/ci/jobs/job_control.html#avoid-duplicate-pipelines\n\nジョブに rules を使用していると、マージリクエスト作成後にブランチに対して push するといった1つのアクションが、push 時に発生したパイプラインとマージリクエストのパイプラインの2つが走らせることが起こりえます。\n\n重複したパイプライン（duplicate pipelines）を避けるためには\n\n* workflow を使ってどの種類のパイプラインは走って良いのかを指定する\n* ジョブが実行される条件をかなり限定的にして、最後のルールとして `when`（`when: never` 以外）を使うのを避ける\n\nことが対策になります。\n\n筆者の場合は、次項で示す例を書いている際に重複したパイプラインが発生し、片方のジョブは正しく機能しないという状況が起こりましたが、workflow を指定することで重複が解消され、正しく機能するようになりました。\n\n## 特定ファイルの変更時のみジョブを実行し、それ以外はスキップして後続のジョブを実行する\n\n`.gitlab-ci.yml` と同階層に `Dockerfile` がある想定です。\n\n```yaml\nstages:\n  - build\n  - test\n\nworkflow:\n  rules:\n    - if: $CI_PIPELINE_SOURCE == \"merge_request_event\"\n    - if: $CI_COMMIT_BRANCH && $CI_OPEN_MERGE_REQUESTS\n      when: never\n    - if: $CI_COMMIT_BRANCH\n\nbuild:\n  stage: build\n  image:\n    name: gcr.io/kaniko-project/executor:v1.9.0-debug\n    entrypoint: [\"\"]\n  script:\n    - /kaniko/executor\n      --context \"${CI_PROJECT_DIR}\"\n      --dockerfile \"${CI_PROJECT_DIR}/Dockerfile\"\n      --destination \"${CI_REGISTRY_IMAGE}:latest\"\n  rules:\n    - if: $CI_PIPELINE_SOURCE =~ /merge_request_event|push/\n      changes:\n        - Dockerfile\n    - when: manual\n      allow_failure: true\n\ntest:\n  stage: test\n  image: \"${CI_REGISTRY_IMAGE}:latest\"\n  script:\n    - echo \"DO SOME TESTS\"\n```\n\n上記の例は以下の挙動をします。\n\n* workflow\n  * マージリクエストでのパイプラインの場合は実行される\n  * マージリクエストがある時、ブランチへの push イベントでのパイプラインは実行されない\n  * それ以外のブランチへの push イベントでは実行される\n* rules\n  * パイプラインがマージリクエストのパイプラインの時または push された時に `Dockerfile` が変更されているか確認する\n    * changes は上記以外では常に真となってしまうため\n  * `Dockerfile` が変更されている時に、ジョブをパイプラインにジョブを追加する\n  * 上記の条件に当てはまらない時はマニュアルジョブとしてパイプラインに追加する\n    * `allow_failure: true` となっているので、後続のジョブはそのまま実行される\n\nこのように書くことで、実現したかった「特定ファイルの変更時のみジョブを実行し、それ以外はスキップして後続のジョブを実行する」が実現します。\n","tags":[{"name":"Gitlab","ref":"/tags/gitlab"},{"name":"CI/CD","ref":"/tags/ci-cd"}]}},"__N_SSG":true}